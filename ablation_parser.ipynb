{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-26T23:27:15.558817Z",
     "start_time": "2026-01-26T23:27:15.543174Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class TempestAblationParser:\n",
    "    def __init__(self, base_dir, batch_count=10):\n",
    "        self.base_dir = base_dir\n",
    "        self.batch_count = batch_count\n",
    "        self.variants = ['v0', 'v1', 'v2', 'v3']\n",
    "        self.results = defaultdict(list)\n",
    "\n",
    "    def clean_numeric(self, value):\n",
    "        \"\"\"Helper to force string values into clean floats.\"\"\"\n",
    "        if value is None or pd.isna(value):\n",
    "            return np.nan\n",
    "        if isinstance(value, (int, float)):\n",
    "            return float(value)\n",
    "        clean_val = str(value).replace(',', '').strip()\n",
    "        try:\n",
    "            return float(clean_val)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    def parse_stdout(self, file_path):\n",
    "        \"\"\"Extracts end-to-end throughput and walks from stdout.txt\"\"\"\n",
    "        metrics = {}\n",
    "        if not os.path.exists(file_path): return metrics\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            # Throughput: e.g., 1.27369e+06 walks/sec\n",
    "            t_match = re.search(r'Throughput:\\s+([\\d.e+-]+)', content)\n",
    "            if t_match: metrics['throughput_ks'] = self.clean_numeric(t_match.group(1)) / 1000\n",
    "            # Total walks\n",
    "            w_match = re.search(r'Total walks:\\s+(\\d+)', content)\n",
    "            if w_match: metrics['total_walks'] = self.clean_numeric(w_match.group(1))\n",
    "        return metrics\n",
    "\n",
    "    def parse_nsys_latencies(self, file_path):\n",
    "        \"\"\"Extracts NVTX latencies (Ingestion, Sort, Rebuild)\"\"\"\n",
    "        metrics = {}\n",
    "        if not os.path.exists(file_path): return metrics\n",
    "        df = pd.read_csv(file_path)\n",
    "        mapping = {\n",
    "            ':ingestion_batch': 'ingest_ms',\n",
    "            ':node_index_rebuild': 'rebuild_ms',\n",
    "            ':ingestion_sort_merge': 'sort_ms'\n",
    "        }\n",
    "        for range_name, key in mapping.items():\n",
    "            row = df[df['Range'] == range_name]\n",
    "            if not row.empty:\n",
    "                val = row['Avg (ns)'].values[0]\n",
    "                metrics[key] = self.clean_numeric(val) / 1e6\n",
    "        return metrics\n",
    "\n",
    "    def parse_nsys_kernels(self, file_path, variant):\n",
    "        \"\"\"Variant-aware kernel parsing (Total Accumulated Time per Batch)\"\"\"\n",
    "        metrics = {}\n",
    "        if not os.path.exists(file_path): return metrics\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if 'v2' in variant: # Step-based logic\n",
    "            # Sum total time of all step kernels and divide by total batches\n",
    "            kernels = df[df['Name'].str.contains('pick_start_edges_kernel|pick_intermediate_edges_kernel', regex=True)]\n",
    "            if not kernels.empty:\n",
    "                total_gpu_ns = kernels['Total Time (ns)'].sum()\n",
    "                metrics['sampling_gpu_ms'] = self.clean_numeric(total_gpu_ns / self.batch_count) / 1e6\n",
    "        else: # Full-walk logic (v0, v1, v3)\n",
    "            row = df[df['Name'].str.contains('generate_random_walks_kernel', na=False)]\n",
    "            if not row.empty:\n",
    "                # For full-walk, one launch = one batch\n",
    "                metrics['sampling_gpu_ms'] = self.clean_numeric(row['Avg (ns)'].values[0]) / 1e6\n",
    "        return metrics\n",
    "\n",
    "    def parse_nsys_memory(self, file_path):\n",
    "        \"\"\"Extracts D2D volume for achieved bandwidth calculation\"\"\"\n",
    "        if not os.path.exists(file_path): return {}\n",
    "        df = pd.read_csv(file_path)\n",
    "        d2d = df[df['Operation'].str.contains('Device-to-Device')]\n",
    "        val = d2d['Total (MB)'].values[0] if not d2d.empty else 0\n",
    "        return {'d2d_mb': self.clean_numeric(val)}\n",
    "\n",
    "    def parse_ncu_metrics(self, file_path):\n",
    "        \"\"\"Extracts Occupancy and Registers\"\"\"\n",
    "        if not os.path.exists(file_path): return {}\n",
    "        df = pd.read_csv(file_path)\n",
    "        data_df = df.apply(pd.to_numeric, errors='coerce').dropna(how='all')\n",
    "        if data_df.empty: return {}\n",
    "\n",
    "        row = data_df.iloc[0]\n",
    "        return {\n",
    "            'occupancy': self.clean_numeric(row.get('sm__maximum_warps_per_active_cycle_pct')),\n",
    "            'registers': self.clean_numeric(row.get('launch__registers_per_thread'))\n",
    "        }\n",
    "\n",
    "    def process_all(self):\n",
    "        \"\"\"Walks the directory structure and collects all metrics.\"\"\"\n",
    "        for run_dir in sorted(os.listdir(self.base_dir)):\n",
    "            run_path = os.path.join(self.base_dir, run_dir)\n",
    "            if not os.path.isdir(run_path) or 'run-' not in run_dir: continue\n",
    "\n",
    "            for var_dir in os.listdir(run_path):\n",
    "                var_path = os.path.join(run_path, var_dir)\n",
    "                variant_id = var_dir.split('_')[0]\n",
    "                if variant_id not in self.variants: continue\n",
    "\n",
    "                m = {}\n",
    "                m.update(self.parse_stdout(os.path.join(var_path, 'stdout.txt')))\n",
    "                m.update(self.parse_nsys_latencies(os.path.join(var_path, 'nsys_rebuild_latencies.csv')))\n",
    "                m.update(self.parse_nsys_kernels(os.path.join(var_path, 'nsys_kernel_summary.csv'), variant_id))\n",
    "                m.update(self.parse_nsys_memory(os.path.join(var_path, 'nsys_memory_transfers.csv')))\n",
    "                m.update(self.parse_ncu_metrics(os.path.join(var_path, 'ncu_raw_metrics.csv')))\n",
    "\n",
    "                # Calculate Achieved HBM Throughput\n",
    "                if 'd2d_mb' in m and 'rebuild_ms' in m and m['rebuild_ms'] > 0:\n",
    "                    bw_gbs = (m['d2d_mb'] / 1024) / (m['rebuild_ms'] / 1000)\n",
    "                    m['achieved_hbm_util'] = (bw_gbs / 1555) * 100 # A100 Peak\n",
    "\n",
    "                self.results[variant_id].append(m)\n",
    "\n",
    "    def summarize(self):\n",
    "        \"\"\"Aggregates across runs to produce Mean Â± Std Dev.\"\"\"\n",
    "        summary = {}\n",
    "        for var in self.variants:\n",
    "            data_list = self.results[var]\n",
    "            if not data_list: continue\n",
    "\n",
    "            df = pd.DataFrame(data_list).apply(pd.to_numeric, errors='coerce')\n",
    "            stats = df.agg(['mean', 'std']).T\n",
    "            summary[var] = stats\n",
    "\n",
    "            print(f\"\\n=== Final Results for {var} ===\")\n",
    "            print(stats[['mean', 'std']])\n",
    "        return summary"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T23:27:16.790054Z",
     "start_time": "2026-01-26T23:27:16.187771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "parser = TempestAblationParser('/Users/ashfaqsalehin/Documents/ablation_results_parsed')\n",
    "parser.process_all()\n",
    "parser.summarize()"
   ],
   "id": "12c7f713f2d68604",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Results for v0 ===\n",
      "                           mean       std\n",
      "throughput_ks      1.274122e+03  4.588695\n",
      "total_walks        2.050816e+07  0.000000\n",
      "ingest_ms          6.290281e+01  0.359079\n",
      "rebuild_ms         5.545217e+00  0.022093\n",
      "sort_ms            3.204124e+00  0.023669\n",
      "sampling_gpu_ms    1.046515e+02  0.036255\n",
      "d2d_mb             7.126649e+03  0.000000\n",
      "occupancy          3.750000e+01  0.000000\n",
      "registers          7.000000e+01  0.000000\n",
      "achieved_hbm_util  8.071272e+01  0.321404\n",
      "\n",
      "=== Final Results for v1 ===\n",
      "                           mean       std\n",
      "throughput_ks      1.282898e+03  5.344134\n",
      "total_walks        2.050816e+07  0.000000\n",
      "ingest_ms          6.334607e+01  0.482846\n",
      "rebuild_ms         5.581059e+00  0.063290\n",
      "sort_ms            3.241443e+00  0.024114\n",
      "sampling_gpu_ms    1.011157e+02  0.031938\n",
      "d2d_mb             7.126649e+03  0.000000\n",
      "occupancy          5.000000e+01  0.000000\n",
      "registers          6.400000e+01  0.000000\n",
      "achieved_hbm_util  8.020158e+01  0.906240\n",
      "\n",
      "=== Final Results for v2 ===\n",
      "                           mean       std\n",
      "throughput_ks      1.261654e+03  6.073239\n",
      "total_walks        2.050816e+07  0.000000\n",
      "ingest_ms          6.268597e+01  0.166414\n",
      "rebuild_ms         5.525304e+00  0.020114\n",
      "sort_ms            3.190030e+00  0.012055\n",
      "sampling_gpu_ms    1.174768e+02  0.033222\n",
      "d2d_mb             7.126649e+03  0.000000\n",
      "occupancy          7.500000e+01  0.000000\n",
      "registers          3.800000e+01  0.000000\n",
      "achieved_hbm_util  8.100343e+01  0.294942\n",
      "\n",
      "=== Final Results for v3 ===\n",
      "                           mean       std\n",
      "throughput_ks      1.239864e+03  4.519605\n",
      "total_walks        2.050816e+07  0.000000\n",
      "ingest_ms          7.438834e+01  0.354003\n",
      "rebuild_ms         5.535124e+00  0.012385\n",
      "sort_ms            3.178988e+00  0.008620\n",
      "sampling_gpu_ms    1.452669e+02  0.041606\n",
      "d2d_mb             8.592592e+03  0.000000\n",
      "occupancy          5.000000e+01  0.000000\n",
      "registers          5.600000e+01  0.000000\n",
      "achieved_hbm_util  9.749183e+01  0.218020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'v0':                            mean       std\n",
       " throughput_ks      1.274122e+03  4.588695\n",
       " total_walks        2.050816e+07  0.000000\n",
       " ingest_ms          6.290281e+01  0.359079\n",
       " rebuild_ms         5.545217e+00  0.022093\n",
       " sort_ms            3.204124e+00  0.023669\n",
       " sampling_gpu_ms    1.046515e+02  0.036255\n",
       " d2d_mb             7.126649e+03  0.000000\n",
       " occupancy          3.750000e+01  0.000000\n",
       " registers          7.000000e+01  0.000000\n",
       " achieved_hbm_util  8.071272e+01  0.321404,\n",
       " 'v1':                            mean       std\n",
       " throughput_ks      1.282898e+03  5.344134\n",
       " total_walks        2.050816e+07  0.000000\n",
       " ingest_ms          6.334607e+01  0.482846\n",
       " rebuild_ms         5.581059e+00  0.063290\n",
       " sort_ms            3.241443e+00  0.024114\n",
       " sampling_gpu_ms    1.011157e+02  0.031938\n",
       " d2d_mb             7.126649e+03  0.000000\n",
       " occupancy          5.000000e+01  0.000000\n",
       " registers          6.400000e+01  0.000000\n",
       " achieved_hbm_util  8.020158e+01  0.906240,\n",
       " 'v2':                            mean       std\n",
       " throughput_ks      1.261654e+03  6.073239\n",
       " total_walks        2.050816e+07  0.000000\n",
       " ingest_ms          6.268597e+01  0.166414\n",
       " rebuild_ms         5.525304e+00  0.020114\n",
       " sort_ms            3.190030e+00  0.012055\n",
       " sampling_gpu_ms    1.174768e+02  0.033222\n",
       " d2d_mb             7.126649e+03  0.000000\n",
       " occupancy          7.500000e+01  0.000000\n",
       " registers          3.800000e+01  0.000000\n",
       " achieved_hbm_util  8.100343e+01  0.294942,\n",
       " 'v3':                            mean       std\n",
       " throughput_ks      1.239864e+03  4.519605\n",
       " total_walks        2.050816e+07  0.000000\n",
       " ingest_ms          7.438834e+01  0.354003\n",
       " rebuild_ms         5.535124e+00  0.012385\n",
       " sort_ms            3.178988e+00  0.008620\n",
       " sampling_gpu_ms    1.452669e+02  0.041606\n",
       " d2d_mb             8.592592e+03  0.000000\n",
       " occupancy          5.000000e+01  0.000000\n",
       " registers          5.600000e+01  0.000000\n",
       " achieved_hbm_util  9.749183e+01  0.218020}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9cbb7d0e4caf5281"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
